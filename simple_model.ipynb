{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, neurons_in_layer, learning_rate):\n",
    "        self.neurons_in_layer = neurons_in_layer   # 1 input layer, 2 hidden, 1 output layer so a 1x4 array\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layers = len(neurons_in_layer) # Excluding final layer\n",
    "\n",
    "        # Initializing weights & biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(self.layers - 1):\n",
    "            # In the matrices rows correspond to neurons in the current layer and columns to neurons in the next layer\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[neurons_in_layer[i],neurons_in_layer[i+1]]))\n",
    "            self.biases.append(np.zeros((1, neurons_in_layer[i+1])))\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "    def ReLU_derivative(self, x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    # To have probabilities (all sum up to 1) in the output layer\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def feed_forward(self, batch):\n",
    "        self.layer_output = [batch]\n",
    "\n",
    "        # Passing through each layer except the last one\n",
    "        for i in range(self.layers - 1):\n",
    "            output = self.layer_output[-1].dot(self.weights[i]) + self.biases[i]   # weight * x + bias\n",
    "            self.layer_output.append(self.ReLU(output))\n",
    "\n",
    "        # Using softmax in the output layer\n",
    "        self.layer_output[-1] = self.softmax(self.layer_output[-1])\n",
    "        return self.layer_output[-1]\n",
    "        #final_x = np.dot(self.layer_output[-1], self.weights[-1]) + self.biases[-1]\n",
    "        #self.x_values.append(final_x)\n",
    "        #self.layer_output.append(self.softmax(final_x))\n",
    "\n",
    "    def backpropagation(self, y):\n",
    "        output_error = self.layer_output[-1] - y  # cross- entropy loss\n",
    "        deltas = [output_error]\n",
    "\n",
    "        # Backpropagation through each layer backwards (duh)\n",
    "        for i in range(self.layers - 3, -1, -1):\n",
    "            delta = deltas[-1].dot(self.weights[i+1].T) * self.ReLU_derivative(self.layer_output[i+1])\n",
    "            deltas.append(delta)\n",
    "\n",
    "        # Reversing deltas to match layer order\n",
    "        deltas.reverse()\n",
    "\n",
    "        # Updating weights and biases\n",
    "        for i in range(self.layers - 1):\n",
    "            self.weights[i] -= self.learning_rate * self.layer_output[i].T.dot(deltas[i])\n",
    "            self.biases[i] -= self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y, epochs = 1000):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.feed_forward(X)\n",
    "            self.backpropagation(y)\n",
    "            print(output)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = -np.sum(y * np.log(output + 1e-9)) / X.shape[0]\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.feed_forward(X)\n",
    "        print(\"Output:\", output)\n",
    "        print(\"Label:\", np.argmax(output, axis=1))\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"C:/Users/afrod/Documents/Neural_Networks/MergedDataset\"\n",
    "classes = [\"NonDemented\", \"VeryMildDemented\", \"MildDemented\", \"ModerateDemented\"]\n",
    "training_data = []\n",
    "\n",
    "\n",
    "def create_training_data():\n",
    "    for dementia_level in classes:\n",
    "        path = os.path.join(data_file, dementia_level)\n",
    "        class_num = classes.index(dementia_level)\n",
    "        for img in os.listdir(path):\n",
    "            # Convert to grayscale for smaller array dimensions\n",
    "            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "            final_array = cv2.resize(img_array, (50,47))\n",
    "            training_data.append([final_array, class_num])\n",
    "\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40384\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training_data)\n",
    "\n",
    "# Separating features and labels\n",
    "# Images are also flattened to be used as input in the knn algorithm\n",
    "X = np.array([features for features, _ in training_data]).reshape(-1, 50*47)\n",
    "y = np.array([label for _, label in training_data])\n",
    "\n",
    "# Rescaling\n",
    "X = X/X.max()\n",
    "\n",
    "# One-hot encoding\n",
    "y_onehot = np.zeros((y.size, int(y.max()) + 1))\n",
    "y_onehot[np.arange(y.size),y.astype(int)] = 1.0\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17808434 0.20538655 0.35662203 0.25990708]\n",
      " [0.22604515 0.16548928 0.35120107 0.25726449]\n",
      " [0.15109094 0.17108427 0.36611508 0.31170971]\n",
      " ...\n",
      " [0.21837117 0.18804728 0.41893405 0.1746475 ]\n",
      " [0.15868902 0.16181326 0.35363458 0.32586313]\n",
      " [0.16864331 0.20599493 0.38820453 0.23715723]]\n",
      "Epoch 0, Loss: 1.4968724058753025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afrod\\AppData\\Local\\Temp\\ipykernel_3336\\2800445351.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1./(1.+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " ...\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]]\n",
      "[[0.36552929 0.13447071 0.36552929 0.13447071]\n",
      " [0.36552929 0.13447071 0.36552929 0.13447071]\n",
      " [0.36552929 0.13447071 0.36552929 0.13447071]\n",
      " ...\n",
      " [0.36552929 0.13447071 0.36552929 0.13447071]\n",
      " [0.36552929 0.13447071 0.36552929 0.13447071]\n",
      " [0.36552929 0.13447071 0.36552929 0.13447071]]\n",
      "[[0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " ...\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]]\n",
      "[[0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " ...\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]]\n",
      "[[0.13447071 0.36552929 0.36552929 0.13447071]\n",
      " [0.13447071 0.36552929 0.36552929 0.13447071]\n",
      " [0.13447071 0.36552929 0.36552929 0.13447071]\n",
      " ...\n",
      " [0.13447071 0.36552929 0.36552929 0.13447071]\n",
      " [0.13447071 0.36552929 0.36552929 0.13447071]\n",
      " [0.13447071 0.36552929 0.36552929 0.13447071]]\n",
      "[[0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " ...\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " [0.47536689 0.1748777  0.1748777  0.1748777 ]]\n",
      "[[0.13466141 0.36604766 0.36462952 0.13466141]\n",
      " [0.13450706 0.3656281  0.36535777 0.13450706]\n",
      " [0.13462319 0.36594377 0.36480984 0.13462319]\n",
      " ...\n",
      " [0.1345569  0.36576358 0.36512262 0.1345569 ]\n",
      " [0.13456265 0.3657792  0.3650955  0.13456265]\n",
      " [0.13456266 0.36577924 0.36509543 0.13456266]]\n",
      "[[0.36577803 0.13456222 0.13456222 0.36509753]\n",
      " [0.36577804 0.13456222 0.13456222 0.36509753]\n",
      " [0.36577804 0.13456222 0.13456222 0.36509752]\n",
      " ...\n",
      " [0.36577804 0.13456222 0.13456222 0.36509752]\n",
      " [0.36577803 0.13456222 0.13456222 0.36509753]\n",
      " [0.36577802 0.13456221 0.13456221 0.36509756]]\n",
      "[[0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " ...\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]\n",
      " [0.36552929 0.36552929 0.13447071 0.13447071]]\n"
     ]
    }
   ],
   "source": [
    "# Define layer sizes for a network with multiple hidden layers\n",
    "neurons_in_layer = [2350, 700, 100, 4]  # 784 input, two hidden layers, 4 possible classes\n",
    "mlp = MLP(neurons_in_layer=neurons_in_layer, learning_rate=0.01)\n",
    "mlp.train(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afrod\\AppData\\Local\\Temp\\ipykernel_3336\\2800445351.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1./(1.+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[0.29692274 0.29692274 0.29692274 0.10923177]\n",
      " [0.29692274 0.29692274 0.29692274 0.10923177]\n",
      " [0.29692274 0.29692274 0.29692274 0.10923177]\n",
      " ...\n",
      " [0.29692274 0.29692274 0.29692274 0.10923177]\n",
      " [0.29692274 0.29692274 0.29692274 0.10923177]\n",
      " [0.29692274 0.29692274 0.29692274 0.10923177]]\n",
      "Label: [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on test set\n",
    "predictions = mlp.predict(X_train)\n",
    "y_test_labels = np.argmax(y_train, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "#print(predictions)\n",
    "#print(\"\\nMLP with layers [2350, 700, 100, 4], lr = 0.01, epochs = 1000:\")\n",
    "#print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "#print(\"Accuracy:\", accuracy_score(y_test_labels, predictions))\n",
    "#print(\"Classification Report:\\n\", classification_report(y_test_labels, predictions))\n",
    "#print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

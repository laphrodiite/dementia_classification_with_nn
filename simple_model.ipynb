{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, neurons_in_layer, learning_rate):\n",
    "        self.neurons_in_layer = neurons_in_layer   # 1 input layer, 2 hidden, 1 output layer so a 1x4 array\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layers = len(neurons_in_layer) # Excluding final layer\n",
    "\n",
    "        # Initializing weights & biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "\n",
    "        for i in range(self.layers - 1):\n",
    "            # In the matrices rows correspond to neurons in the current layer and columns to neurons in the next layer\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[neurons_in_layer[i],neurons_in_layer[i+1]]))\n",
    "            self.biases.append(np.zeros((1, neurons_in_layer[i+1])))\n",
    "\n",
    "        print(len(self.weights[0]))\n",
    "        print(len(self.weights[1]))\n",
    "        print(len(self.weights[2]))    \n",
    "    def sigmoid(self, x):\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        sigmoid_x = self.sigmoid(x)\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def ReLU_derivative(self, x):\n",
    "        return (x>0)*1\n",
    "    \n",
    "    # To have probabilities (all sum up to 1) in the output layer\n",
    "    def softmax(self, x):\n",
    "        print(\"x:\", x)\n",
    "        exp_x = np.exp(x)\n",
    "        print(\"exp_x:\", exp_x)\n",
    "        prob = exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "        print(\"prob:\", prob)\n",
    "        return prob\n",
    "    \n",
    "    def feed_forward(self, batch):\n",
    "        self.layer_output = [batch]\n",
    "\n",
    "        # Passing through each layer except the last one\n",
    "        for i in range(self.layers - 1):\n",
    "            output = self.layer_output[-1].dot(self.weights[i]) + self.biases[i]   # weight * x + bias\n",
    "            self.layer_output.append(self.ReLU(output))\n",
    "\n",
    "        # Using softmax in the output layer\n",
    "        self.layer_output[-1] = self.softmax(self.layer_output[-1])\n",
    "        \n",
    "        print(self.layer_output[-1])\n",
    "        return self.layer_output[-1]\n",
    "        #final_x = np.dot(self.layer_output[-1], self.weights[-1]) + self.biases[-1]\n",
    "        #self.x_values.append(final_x)\n",
    "        #self.layer_output.append(self.softmax(final_x))\n",
    "\n",
    "    def backpropagation(self, y, X_size):\n",
    "        output_error = self.layer_output[-1] - y  # cross- entropy loss\n",
    "        delta = output_error * self.ReLU_derivative(self.layer_output[-1])\n",
    "\n",
    "        # Backpropagation through each layer backwards (duh)\n",
    "        for i in range(1, self.layers - 1):\n",
    "            self.weights[-i] -= self.learning_rate * (self.layer_output[-i-1].T.dot(delta))\n",
    "            self.biases[-i] -= self.learning_rate * np.mean(delta, axis=0, keepdims=True)\n",
    "            delta = self.ReLU_derivative(self.layer_output[-i-1]) * (delta.dot(self.weights[-i].T))\n",
    "            # delta = deltas[-1].dot(self.weights[i+1].T) * self.ReLU_derivative(self.layer_output[i+1])\n",
    "            # deltas.append(delta)\n",
    "\n",
    "        # # Reversing deltas to match layer order\n",
    "        # deltas.reverse()\n",
    "\n",
    "        # # Updating weights and biases\n",
    "        # for i in range(self.layers - 1):\n",
    "        #     self.weights[i] -= self.learning_rate * self.layer_output[i].T.dot(deltas[i])\n",
    "        #     self.biases[i] -= self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y, epochs = 1000):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.feed_forward(X)\n",
    "            self.backpropagation(y, len(X))\n",
    "            #print(output)\n",
    "            if epoch % 1 == 0:\n",
    "                loss = -np.sum(y * np.log(output + 1e-9)) / X.shape[0]\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.feed_forward(X)\n",
    "        print(\"Output:\", output)\n",
    "        print(\"Label:\", np.argmax(output, axis=1))\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_new:\n",
    "    def __init__(self, input, hidden_layer1, hidden_layer2, output, learning_rate, sample_size):\n",
    "        self.input = input\n",
    "        self.hidden_layer1 = hidden_layer1\n",
    "        self.hidden_layer2 = hidden_layer2\n",
    "        self.output = output\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.sample_size = sample_size\n",
    "        self.total_error = 0\n",
    "\n",
    "        # Initializing weights and biases\n",
    "        self.weights1 = np.random.uniform(-1, 1, size=[input, hidden_layer1])\n",
    "        self.weights2 = np.random.uniform(-1, 1, size=[hidden_layer1, hidden_layer2])\n",
    "        self.weights3 = np.random.uniform(-1, 1, size=[hidden_layer2, output])\n",
    "\n",
    "        self.biases1 = np.zeros((1, hidden_layer1))\n",
    "        self.biases2 = np.zeros((1, hidden_layer2))\n",
    "        self.biases3 = np.zeros((1, output))\n",
    "\n",
    "        self.final_output = []\n",
    "        self.loss = []\n",
    "\n",
    "\n",
    "    # Defining useful functions\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x * 1.0))\n",
    "        \n",
    "    def sigmoid_prime(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def ReLU_prime(self,x):\n",
    "        return (x>0)*1\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp = np.exp(x_shifted)\n",
    "        probabilities = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "    \n",
    "    def cross_entropy_loss(self, y):\n",
    "        loss = 0\n",
    "        for sample in range(y.shape[0]):\n",
    "            final_output_label = np.argmax(self.final_output[sample])\n",
    "            y_label = np.argmax(y[sample])\n",
    "            \n",
    "            loss = loss + (-1 * y_label*np.log(final_output_label+ 1e-9))\n",
    "        return (loss/y.shape[0])\n",
    "    \n",
    "    def return_loss(self):\n",
    "        return self.loss\n",
    "\n",
    "    # Forward\n",
    "    def train(self, X, y, epochs):\n",
    "        self.loss = np.zeros([epochs,1])\n",
    "        for epoch in range(epochs):\n",
    "            self.final_output = np.zeros([self.sample_size, self.output])\n",
    "            for sample, inputs in enumerate(X):\n",
    "                inputs = inputs.reshape(-1,1).T\n",
    "                # Forward pass\n",
    "                output1 = self.sigmoid(inputs.dot(self.weights1) + self.biases1)\n",
    "                output2 = self.sigmoid(output1.dot(self.weights2) + self.biases2)\n",
    "                output3 = self.softmax(output2.dot(self.weights3) + self.biases3)\n",
    "\n",
    "                self.final_output[sample] = output3\n",
    "\n",
    "                # Backward pass\n",
    "                error = output3 - y[sample]\n",
    "                self.total_error += error\n",
    "                \n",
    "                delta_out = error * self.sigmoid_prime(output3)\n",
    "                self.weights3 -= self.learning_rate * (output2.T.dot(delta_out))\n",
    "                self.biases3 -= self.learning_rate * delta_out\n",
    "        \n",
    "                delta2 = self.sigmoid_prime(output2) * (delta_out.dot(self.weights3.T))\n",
    "                self.weights2 -= self.learning_rate * (output1.T.dot(delta2))\n",
    "                self.biases2 -= self.learning_rate * delta2\n",
    "\n",
    "                delta1 = self.sigmoid_prime(output1) * (delta2.dot(self.weights2.T))\n",
    "                self.weights1 -= self.learning_rate * (inputs.T.dot(delta1))\n",
    "                self.biases1 -= self.learning_rate * delta1\n",
    "\n",
    "            self.total_error = self.total_error/len(y)\n",
    "\n",
    "            self.loss[epoch] = self.cross_entropy_loss(y)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {self.loss[epoch]}, Final output: {self.final_output}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        self.final_output = np.zeros([self.sample_size, self.output])\n",
    "        for sample, inputs in enumerate(X):\n",
    "            # Forward pass\n",
    "            inputs = inputs.reshape(-1,1).T\n",
    "                # Forward pass\n",
    "            output1 = self.sigmoid(inputs.dot(self.weights1) + self.biases1)\n",
    "            output2 = self.sigmoid(output1.dot(self.weights2) + self.biases2)\n",
    "            output3 = self.softmax(output2.dot(self.weights3) + self.biases3)\n",
    "\n",
    "            self.final_output[sample] = output3\n",
    "\n",
    "            predictions.append(np.argmax(self.final_output[sample]))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"C:/Users/afrod/Documents/Neural_Networks/MergedDataset\"\n",
    "classes = [\"NonDemented\", \"VeryMildDemented\", \"MildDemented\", \"ModerateDemented\"]\n",
    "training_data = []\n",
    "\n",
    "\n",
    "def create_training_data():\n",
    "    for dementia_level in classes:\n",
    "        path = os.path.join(data_file, dementia_level)\n",
    "        class_num = classes.index(dementia_level)\n",
    "        for img in os.listdir(path):\n",
    "            # Convert to grayscale for smaller array dimensions\n",
    "            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "            final_array = cv2.resize(img_array, (50,47))\n",
    "            training_data.append([final_array, class_num])\n",
    "\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24230\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training_data)\n",
    "\n",
    "# Separating features and labels\n",
    "# Images are also flattened to be used as input in the knn algorithm\n",
    "X = np.array([features for features, _ in training_data]).reshape(-1, 50*47)\n",
    "y = np.array([label for _, label in training_data])\n",
    "\n",
    "# Rescaling\n",
    "X = X/X.max()\n",
    "\n",
    "# One-hot encoding\n",
    "y_onehot = np.zeros((y.size, int(y.max()) + 1))\n",
    "y_onehot[np.arange(y.size),y.astype(int)] = 1.0\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.4, random_state=42)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_new(2350, 1000, 100, 4, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: [7.01968192], Final output: [[1.14329059e-01 5.45055231e-02 8.31122229e-01 4.31884423e-05]\n",
      " [3.53365735e-02 4.47625396e-02 9.19874124e-01 2.67633677e-05]\n",
      " [3.23935367e-02 3.96081761e-02 9.27970960e-01 2.73274051e-05]\n",
      " ...\n",
      " [1.48180948e-01 4.28709168e-01 3.73400101e-01 4.97097822e-02]\n",
      " [2.08409521e-01 3.60298427e-01 4.05783202e-01 2.55088501e-02]\n",
      " [8.87139876e-02 4.15186685e-01 3.23342994e-01 1.72756333e-01]]\n"
     ]
    }
   ],
   "source": [
    "mlp.train(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP with layers [2350, 600, 100, 4], lr = 0.01, epochs = 100:\n",
      "Test Accuracy: 39.96%\n",
      "Accuracy: 0.39959143246254797\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.02      0.04      5078\n",
      "           1       0.32      0.42      0.36      4448\n",
      "           2       0.35      0.72      0.47      3978\n",
      "           3       0.83      0.60      0.70      2650\n",
      "\n",
      "    accuracy                           0.40     16154\n",
      "   macro avg       0.56      0.44      0.39     16154\n",
      "weighted avg       0.55      0.40      0.34     16154\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  99 3040 1891   48]\n",
      " [  17 1881 2429  121]\n",
      " [  14  918 2879  167]\n",
      " [   0   94  960 1596]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on test set\n",
    "predictions = mlp.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(\"\\nMLP with layers [2350, 600, 100, 4], lr = 0.01, epochs = 100:\")\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.71270071],\n",
       "       [6.12670058]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, neurons_in_layer, learning_rate):\n",
    "        self.neurons_in_layer = neurons_in_layer   # 1 input layer, 2 hidden, 1 output layer so a 1x4 array\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layers = len(neurons_in_layer) # Excluding final layer\n",
    "\n",
    "        # Initializing weights & biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "\n",
    "        for i in range(self.layers - 1):\n",
    "            # In the matrices rows correspond to neurons in the current layer and columns to neurons in the next layer\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[neurons_in_layer[i],neurons_in_layer[i+1]]))\n",
    "            self.biases.append(np.zeros((1, neurons_in_layer[i+1])))\n",
    "\n",
    "        print(len(self.weights[0]))\n",
    "        print(len(self.weights[1]))\n",
    "        print(len(self.weights[2]))    \n",
    "    def sigmoid(self, x):\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        sigmoid_x = self.sigmoid(x)\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def ReLU_derivative(self, x):\n",
    "        return (x>0)*1\n",
    "    \n",
    "    # To have probabilities (all sum up to 1) in the output layer\n",
    "    def softmax(self, x):\n",
    "        print(\"x:\", x)\n",
    "        exp_x = np.exp(x)\n",
    "        print(\"exp_x:\", exp_x)\n",
    "        prob = exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "        print(\"prob:\", prob)\n",
    "        return prob\n",
    "    \n",
    "    def feed_forward(self, batch):\n",
    "        self.layer_output = [batch]\n",
    "\n",
    "        # Passing through each layer except the last one\n",
    "        for i in range(self.layers - 1):\n",
    "            output = self.layer_output[-1].dot(self.weights[i]) + self.biases[i]   # weight * x + bias\n",
    "            self.layer_output.append(self.ReLU(output))\n",
    "\n",
    "        # Using softmax in the output layer\n",
    "        self.layer_output[-1] = self.softmax(self.layer_output[-1])\n",
    "        \n",
    "        print(self.layer_output[-1])\n",
    "        return self.layer_output[-1]\n",
    "        #final_x = np.dot(self.layer_output[-1], self.weights[-1]) + self.biases[-1]\n",
    "        #self.x_values.append(final_x)\n",
    "        #self.layer_output.append(self.softmax(final_x))\n",
    "\n",
    "    def backpropagation(self, y, X_size):\n",
    "        output_error = self.layer_output[-1] - y  # cross- entropy loss\n",
    "        delta = output_error * self.ReLU_derivative(self.layer_output[-1])\n",
    "\n",
    "        # Backpropagation through each layer backwards (duh)\n",
    "        for i in range(1, self.layers - 1):\n",
    "            self.weights[-i] -= self.learning_rate * (self.layer_output[-i-1].T.dot(delta))\n",
    "            self.biases[-i] -= self.learning_rate * np.mean(delta, axis=0, keepdims=True)\n",
    "            delta = self.ReLU_derivative(self.layer_output[-i-1]) * (delta.dot(self.weights[-i].T))\n",
    "            # delta = deltas[-1].dot(self.weights[i+1].T) * self.ReLU_derivative(self.layer_output[i+1])\n",
    "            # deltas.append(delta)\n",
    "\n",
    "        # # Reversing deltas to match layer order\n",
    "        # deltas.reverse()\n",
    "\n",
    "        # # Updating weights and biases\n",
    "        # for i in range(self.layers - 1):\n",
    "        #     self.weights[i] -= self.learning_rate * self.layer_output[i].T.dot(deltas[i])\n",
    "        #     self.biases[i] -= self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y, epochs = 1000):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.feed_forward(X)\n",
    "            self.backpropagation(y, len(X))\n",
    "            #print(output)\n",
    "            if epoch % 1 == 0:\n",
    "                loss = -np.sum(y * np.log(output + 1e-9)) / X.shape[0]\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.feed_forward(X)\n",
    "        print(\"Output:\", output)\n",
    "        print(\"Label:\", np.argmax(output, axis=1))\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_new:\n",
    "    def __init__(self, input, hidden_layer1, hidden_layer2, output, learning_rate, sample_size):\n",
    "        self.input = input\n",
    "        self.hidden_layer1 = hidden_layer1\n",
    "        self.hidden_layer2 = hidden_layer2\n",
    "        self.output = output\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.sample_size = sample_size\n",
    "        self.total_error = 0\n",
    "\n",
    "        # Initializing weights and biases\n",
    "        self.weights1 = np.random.uniform(-1, 1, size=[input, hidden_layer1])\n",
    "        self.weights2 = np.random.uniform(-1, 1, size=[hidden_layer1, hidden_layer2])\n",
    "        self.weights3 = np.random.uniform(-1, 1, size=[hidden_layer2, output])\n",
    "\n",
    "        self.biases1 = np.zeros((1, hidden_layer1))\n",
    "        self.biases2 = np.zeros((1, hidden_layer2))\n",
    "        self.biases3 = np.zeros((1, output))\n",
    "\n",
    "        self.final_output = []\n",
    "        self.loss = []\n",
    "\n",
    "\n",
    "    # Defining useful functions\n",
    "    def sigmoid(self, x):\n",
    "        x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        return 1/(1+np.exp(-x_shifted * 1.0))\n",
    "        \n",
    "    def sigmoid_prime(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def ReLU_prime(self,x):\n",
    "        return (x>0)*1\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp = np.exp(x_shifted)\n",
    "        probabilities = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "    \n",
    "    def cross_entropy_loss(self, y):\n",
    "        loss = 0\n",
    "        # Loop through each sample\n",
    "        for sample in range(y.shape[0]):\n",
    "            # Get the predicted probabilities for the current sample\n",
    "            probabilities = self.final_output[sample]\n",
    "            \n",
    "            # Ensure numerical stability by adding a small constant (epsilon) to avoid log(0)\n",
    "            epsilon = 1e-9\n",
    "            probabilities = np.clip(probabilities, epsilon, 1 - epsilon)\n",
    "            \n",
    "            # Compute the loss for the current sample using the correct one-hot encoded label\n",
    "            loss += -np.sum(y[sample] * np.log(probabilities))\n",
    "        \n",
    "        # Return the average loss\n",
    "        return loss / y.shape[0]\n",
    "\n",
    "    # Forward\n",
    "    def train(self, X, y, epochs):\n",
    "        self.loss = np.zeros([epochs,1])\n",
    "        for epoch in range(epochs):\n",
    "            self.final_output = np.zeros([self.sample_size, self.output])\n",
    "            for sample, inputs in enumerate(X):\n",
    "                inputs = inputs.reshape(-1,1).T\n",
    "                # Forward pass\n",
    "                output1 = self.sigmoid(inputs.dot(self.weights1) + self.biases1)\n",
    "                output2 = self.sigmoid(output1.dot(self.weights2) + self.biases2)\n",
    "                output3 = self.softmax(output2.dot(self.weights3) + self.biases3)\n",
    "\n",
    "                self.final_output[sample] = output3\n",
    "\n",
    "                # Backward pass\n",
    "                error = output3 - y[sample]\n",
    "                self.total_error += error\n",
    "                \n",
    "                delta_out = error\n",
    "                self.weights3 -= self.learning_rate * (output2.T.dot(delta_out))\n",
    "                self.biases3 -= self.learning_rate * delta_out\n",
    "        \n",
    "                delta2 = self.sigmoid_prime(output2) * (delta_out.dot(self.weights3.T))\n",
    "                self.weights2 -= self.learning_rate * (output1.T.dot(delta2))\n",
    "                self.biases2 -= self.learning_rate * delta2\n",
    "\n",
    "                delta1 = self.sigmoid_prime(output1) * (delta2.dot(self.weights2.T))\n",
    "                self.weights1 -= self.learning_rate * (inputs.T.dot(delta1))\n",
    "                self.biases1 -= self.learning_rate * delta1\n",
    "\n",
    "            self.total_error = self.total_error/len(y)\n",
    "\n",
    "            self.loss[epoch] = self.cross_entropy_loss(y)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {self.loss[epoch]}, Final output: {self.final_output}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        self.final_output = np.zeros([self.sample_size, self.output])\n",
    "        for sample, inputs in enumerate(X):\n",
    "            # Forward pass\n",
    "            inputs = inputs.reshape(-1,1).T\n",
    "                # Forward pass\n",
    "            output1 = self.sigmoid(inputs.dot(self.weights1) + self.biases1)\n",
    "            output2 = self.sigmoid(output1.dot(self.weights2) + self.biases2)\n",
    "            output3 = self.softmax(output2.dot(self.weights3) + self.biases3)\n",
    "\n",
    "            self.final_output[sample] = output3\n",
    "\n",
    "            predictions.append(np.argmax(self.final_output[sample]))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_new_batches:\n",
    "    def __init__(self, input_size, hidden_layer1, hidden_layer2, output_size, learning_rate, batch_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer1 = hidden_layer1\n",
    "        self.hidden_layer2 = hidden_layer2\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.loss = []\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights1 = np.random.uniform(-1, 1, size=(input_size, hidden_layer1))\n",
    "        self.weights2 = np.random.uniform(-1, 1, size=(hidden_layer1, hidden_layer2))\n",
    "        self.weights3 = np.random.uniform(-1, 1, size=(hidden_layer2, output_size))\n",
    "\n",
    "        self.biases1 = np.zeros((1, hidden_layer1))\n",
    "        self.biases2 = np.zeros((1, hidden_layer2))\n",
    "        self.biases3 = np.zeros((1, output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_prime(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def ReLU_prime(self,x):\n",
    "        return (x>0)*1\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp = np.exp(x_shifted)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        epsilon = 1e-9\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        self.loss = np.zeros([epochs, 1])\n",
    "        num_samples = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            # # Shuffle data\n",
    "            # indices = np.arange(num_samples)\n",
    "            # np.random.shuffle(indices)\n",
    "            # X = X[indices]\n",
    "            # y = y[indices]\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, num_samples, self.batch_size):\n",
    "                # Get batch\n",
    "                X_batch = X[i:i + self.batch_size]\n",
    "                y_batch = y[i:i + self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                z1 = X_batch.dot(self.weights1) + self.biases1\n",
    "                a1 = self.ReLU(z1)\n",
    "\n",
    "                z2 = a1.dot(self.weights2) + self.biases2\n",
    "                a2 = self.ReLU(z2)\n",
    "\n",
    "                z3 = a2.dot(self.weights3) + self.biases3\n",
    "                a3 = self.softmax(z3)\n",
    "\n",
    "                # Loss calculation\n",
    "                batch_loss = self.cross_entropy_loss(a3, y_batch)\n",
    "                epoch_loss += batch_loss\n",
    "\n",
    "                # Backward pass\n",
    "                error_out = a3 - y_batch\n",
    "                grad_weights3 = a2.T.dot(error_out) / self.batch_size\n",
    "                grad_biases3 = np.sum(error_out, axis=0, keepdims=True) / self.batch_size\n",
    "\n",
    "                error_hidden2 = error_out.dot(self.weights3.T) * self.ReLU_prime(z2)\n",
    "                grad_weights2 = a1.T.dot(error_hidden2) / self.batch_size\n",
    "                grad_biases2 = np.sum(error_hidden2, axis=0, keepdims=True) / self.batch_size\n",
    "\n",
    "                error_hidden1 = error_hidden2.dot(self.weights2.T) * self.ReLU_prime(z1)\n",
    "                grad_weights1 = X_batch.T.dot(error_hidden1) / self.batch_size\n",
    "                grad_biases1 = np.sum(error_hidden1, axis=0, keepdims=True) / self.batch_size\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.weights3 -= self.learning_rate * grad_weights3\n",
    "                self.biases3 -= self.learning_rate * grad_biases3\n",
    "\n",
    "                self.weights2 -= self.learning_rate * grad_weights2\n",
    "                self.biases2 -= self.learning_rate * grad_biases2\n",
    "\n",
    "                self.weights1 -= self.learning_rate * grad_weights1\n",
    "                self.biases1 -= self.learning_rate * grad_biases1\n",
    "\n",
    "            self.loss[epoch] = epoch_loss / (num_samples / self.batch_size)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / (num_samples / self.batch_size):.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        z1 = X.dot(self.weights1) + self.biases1\n",
    "        a1 = self.ReLU(z1)\n",
    "\n",
    "        z2 = a1.dot(self.weights2) + self.biases2\n",
    "        a2 = self.ReLU(z2)\n",
    "\n",
    "        z3 = a2.dot(self.weights3) + self.biases3\n",
    "        a3 = self.softmax(z3)\n",
    "\n",
    "        return np.argmax(a3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"C:/Users/afrod/Documents/Neural_Networks/MergedDataset\"\n",
    "classes = [\"NonDemented\", \"VeryMildDemented\", \"MildDemented\", \"ModerateDemented\"]\n",
    "training_data = []\n",
    "\n",
    "\n",
    "def create_training_data():\n",
    "    for dementia_level in classes:\n",
    "        path = os.path.join(data_file, dementia_level)\n",
    "        class_num = classes.index(dementia_level)\n",
    "        for img in os.listdir(path):\n",
    "            # Convert to grayscale for smaller array dimensions\n",
    "            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "            final_array = cv2.resize(img_array, (100,95))\n",
    "            training_data.append([final_array, class_num])\n",
    "\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24230\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training_data)\n",
    "\n",
    "# Separating features and labels\n",
    "# Images are also flattened to be used as input in the knn algorithm\n",
    "X = np.array([features for features, _ in training_data]).reshape(-1, 100*95)\n",
    "y = np.array([label for _, label in training_data])\n",
    "\n",
    "# Rescaling\n",
    "X = (X-X.min())/(X.max() - X.min())\n",
    "\n",
    "# One-hot encoding\n",
    "y_onehot = np.zeros((y.size, int(y.max()) + 1))\n",
    "y_onehot[np.arange(y.size),y.astype(int)] = 1.0\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.4, random_state=42)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_new(2350, 100, 100, 4, 0.001, len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.train(X_train, y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate accuracy on test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y_test_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(predictions \u001b[38;5;241m==\u001b[39m y_test_labels)\n",
      "Cell \u001b[1;32mIn[55], line 105\u001b[0m, in \u001b[0;36mMLP_new_batches.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    102\u001b[0m z1 \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights1) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases1\n\u001b[0;32m    103\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(z1)\n\u001b[1;32m--> 105\u001b[0m z2 \u001b[38;5;241m=\u001b[39m \u001b[43ma1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases2\n\u001b[0;32m    106\u001b[0m a2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(z2)\n\u001b[0;32m    108\u001b[0m z3 \u001b[38;5;241m=\u001b[39m a2\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights3) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases3\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on test set\n",
    "predictions = mlp.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(\"\\nMLP with layers [2350, 100, 100, 4], lr = 0.001, epochs = 50:\")\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mat = np.matrix(mlp.loss)\n",
    "\n",
    "with open('n_100_0.01_50.txt','wb') as f:\n",
    "    for line in loss_mat:\n",
    "        np.savetxt(f, line, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP with layers [36100, 100, 100, 4], lr = 0.001, epochs = 50:\n",
      "Size: 100x95\n",
      "Train Accuracy: 31.44%\n",
      "Accuracy: 0.3143623607098638\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.47      0.41      7637\n",
      "           1       0.29      0.29      0.29      6744\n",
      "           2       0.31      0.26      0.28      5911\n",
      "           3       0.21      0.13      0.16      3938\n",
      "\n",
      "    accuracy                           0.31     24230\n",
      "   macro avg       0.29      0.29      0.29     24230\n",
      "weighted avg       0.30      0.31      0.30     24230\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3592 2226 1124  695]\n",
      " [2763 1972 1392  617]\n",
      " [2278 1471 1547  615]\n",
      " [1292 1204  936  506]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on test set\n",
    "predictions = mlp.predict(X_train)\n",
    "y_test_labels = np.argmax(y_train, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(\"\\nMLP with layers [36100, 100, 100, 4], lr = 0.001, epochs = 50:\")\n",
    "print(\"Size: 100x95\")\n",
    "print(f'Train Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_new_batches(9500, 500, 500, 4, 0.001, 1000) #2350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 15.2152\n",
      "Epoch 11/100, Loss: 10.3104\n",
      "Epoch 21/100, Loss: 9.2140\n",
      "Epoch 31/100, Loss: 8.4632\n",
      "Epoch 41/100, Loss: 7.3742\n",
      "Epoch 51/100, Loss: 6.1614\n",
      "Epoch 61/100, Loss: 4.8171\n",
      "Epoch 71/100, Loss: 3.9106\n",
      "Epoch 81/100, Loss: 3.3165\n",
      "Epoch 91/100, Loss: 2.8881\n"
     ]
    }
   ],
   "source": [
    "mlp.train(X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.21523039],\n",
       "       [13.48855947],\n",
       "       [12.54538391],\n",
       "       [11.84098091],\n",
       "       [11.49623096],\n",
       "       [11.18797977],\n",
       "       [11.10737227],\n",
       "       [10.68867117],\n",
       "       [10.75512428],\n",
       "       [10.49354465],\n",
       "       [10.31037003],\n",
       "       [10.15610178],\n",
       "       [ 9.97409187],\n",
       "       [ 9.93743856],\n",
       "       [ 9.88742857],\n",
       "       [ 9.66304508],\n",
       "       [ 9.53116878],\n",
       "       [ 9.24315332],\n",
       "       [ 9.50111403],\n",
       "       [ 9.18220717],\n",
       "       [ 9.21403813],\n",
       "       [ 9.02749481],\n",
       "       [ 9.02609293],\n",
       "       [ 8.91917904],\n",
       "       [ 8.78263556],\n",
       "       [ 8.75563261],\n",
       "       [ 8.55130505],\n",
       "       [ 8.63630485],\n",
       "       [ 8.41981368],\n",
       "       [ 8.62965541],\n",
       "       [ 8.4632166 ],\n",
       "       [ 8.35534938],\n",
       "       [ 8.24003727],\n",
       "       [ 8.15382079],\n",
       "       [ 8.04375002],\n",
       "       [ 7.93632471],\n",
       "       [ 7.82244011],\n",
       "       [ 7.72476661],\n",
       "       [ 7.60292927],\n",
       "       [ 7.52107308],\n",
       "       [ 7.3742165 ],\n",
       "       [ 7.29143192],\n",
       "       [ 7.15984279],\n",
       "       [ 7.04506432],\n",
       "       [ 6.91291764],\n",
       "       [ 6.7741967 ],\n",
       "       [ 6.72950421],\n",
       "       [ 6.58225097],\n",
       "       [ 6.4450576 ],\n",
       "       [ 6.29611885],\n",
       "       [ 6.16136859],\n",
       "       [ 6.01955421],\n",
       "       [ 5.87590575],\n",
       "       [ 5.74171799],\n",
       "       [ 5.60034009],\n",
       "       [ 5.45341386],\n",
       "       [ 5.31015527],\n",
       "       [ 5.17090262],\n",
       "       [ 5.04777667],\n",
       "       [ 4.93140955],\n",
       "       [ 4.81711298],\n",
       "       [ 4.70877558],\n",
       "       [ 4.61619828],\n",
       "       [ 4.51494936],\n",
       "       [ 4.41959329],\n",
       "       [ 4.32506643],\n",
       "       [ 4.23119918],\n",
       "       [ 4.13998826],\n",
       "       [ 4.07161514],\n",
       "       [ 4.01705774],\n",
       "       [ 3.91061501],\n",
       "       [ 3.85765211],\n",
       "       [ 3.77560001],\n",
       "       [ 3.72458921],\n",
       "       [ 3.64863926],\n",
       "       [ 3.60413625],\n",
       "       [ 3.52780627],\n",
       "       [ 3.48525652],\n",
       "       [ 3.41581542],\n",
       "       [ 3.37062104],\n",
       "       [ 3.31645988],\n",
       "       [ 3.27074294],\n",
       "       [ 3.22376889],\n",
       "       [ 3.17853688],\n",
       "       [ 3.13362697],\n",
       "       [ 3.08913267],\n",
       "       [ 3.04663019],\n",
       "       [ 3.00497851],\n",
       "       [ 2.96526162],\n",
       "       [ 2.92647848],\n",
       "       [ 2.88814775],\n",
       "       [ 2.85092454],\n",
       "       [ 2.81548363],\n",
       "       [ 2.78080869],\n",
       "       [ 2.7463735 ],\n",
       "       [ 2.71201875],\n",
       "       [ 2.6785215 ],\n",
       "       [ 2.64613181],\n",
       "       [ 2.61482632],\n",
       "       [ 2.58385049]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mat = np.matrix(mlp.loss)\n",
    "\n",
    "with open('100x95-500;500-0.001-100-ReLUloss.txt','wb') as f:\n",
    "    for line in loss_mat:\n",
    "        np.savetxt(f, line, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP with ReLU and layers [9500, 500, 500, 4], lr = 0.01, epochs = 100:\n",
      "Test Accuracy: 48.44%\n",
      "Accuracy: 0.4844001485700136\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.39      0.48      5163\n",
      "           1       0.39      0.47      0.42      4456\n",
      "           2       0.43      0.51      0.47      3945\n",
      "           3       0.60      0.65      0.63      2590\n",
      "\n",
      "    accuracy                           0.48     16154\n",
      "   macro avg       0.51      0.51      0.50     16154\n",
      "weighted avg       0.51      0.48      0.49     16154\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2012 1858  974  319]\n",
      " [ 793 2099 1179  385]\n",
      " [ 343 1179 2018  405]\n",
      " [ 109  289  496 1696]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on test set\n",
    "predictions = mlp.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(\"\\nMLP with ReLU and layers [9500, 500, 500, 4], lr = 0.01, epochs = 100:\")\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP with ReLU and layers [9500, 500, 500, 4], lr = 0.01, epochs = 100:\n",
      "Train Accuracy: 50.07%\n",
      "Accuracy: 0.500660338423442\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.40      0.49      7637\n",
      "           1       0.40      0.50      0.44      6744\n",
      "           2       0.44      0.51      0.47      5911\n",
      "           3       0.66      0.69      0.68      3938\n",
      "\n",
      "    accuracy                           0.50     24230\n",
      "   macro avg       0.53      0.52      0.52     24230\n",
      "weighted avg       0.52      0.50      0.50     24230\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3050 2734 1485  368]\n",
      " [1177 3346 1725  496]\n",
      " [ 538 1824 3002  547]\n",
      " [ 129  455  621 2733]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on test set\n",
    "predictions = mlp.predict(X_train)\n",
    "y_test_labels = np.argmax(y_train, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(\"\\nMLP with ReLU and layers [9500, 500, 500, 4], lr = 0.01, epochs = 100:\")\n",
    "print(f'Train Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(mlp.loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

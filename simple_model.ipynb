{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, neurons_in_layer, learning_rate):\n",
    "        self.neurons_in_layer = neurons_in_layer   # 1 input layer, 2 hidden, 1 output layer so a 1x4 array\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layers = len(neurons_in_layer) # Excluding final layer\n",
    "\n",
    "        # Initializing weights & biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "\n",
    "        for i in range(self.layers - 1):\n",
    "            # In the matrices rows correspond to neurons in the current layer and columns to neurons in the next layer\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[neurons_in_layer[i],neurons_in_layer[i+1]]))\n",
    "            self.biases.append(np.zeros((1, neurons_in_layer[i+1])))\n",
    "\n",
    "        print(len(self.weights[0]))\n",
    "        print(len(self.weights[1]))\n",
    "        print(len(self.weights[2]))    \n",
    "    def sigmoid(self, x):\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        sigmoid_x = self.sigmoid(x)\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def ReLU_derivative(self, x):\n",
    "        return (x>0)*1\n",
    "    \n",
    "    # To have probabilities (all sum up to 1) in the output layer\n",
    "    def softmax(self, x):\n",
    "        print(\"x:\", x)\n",
    "        exp_x = np.exp(x)\n",
    "        print(\"exp_x:\", exp_x)\n",
    "        prob = exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "        print(\"prob:\", prob)\n",
    "        return prob\n",
    "    \n",
    "    def feed_forward(self, batch):\n",
    "        self.layer_output = [batch]\n",
    "\n",
    "        # Passing through each layer except the last one\n",
    "        for i in range(self.layers - 1):\n",
    "            output = self.layer_output[-1].dot(self.weights[i]) + self.biases[i]   # weight * x + bias\n",
    "            self.layer_output.append(self.ReLU(output))\n",
    "\n",
    "        # Using softmax in the output layer\n",
    "        self.layer_output[-1] = self.softmax(self.layer_output[-1])\n",
    "        \n",
    "        print(self.layer_output[-1])\n",
    "        return self.layer_output[-1]\n",
    "        #final_x = np.dot(self.layer_output[-1], self.weights[-1]) + self.biases[-1]\n",
    "        #self.x_values.append(final_x)\n",
    "        #self.layer_output.append(self.softmax(final_x))\n",
    "\n",
    "    def backpropagation(self, y, X_size):\n",
    "        output_error = self.layer_output[-1] - y  # cross- entropy loss\n",
    "        delta = output_error * self.ReLU_derivative(self.layer_output[-1])\n",
    "\n",
    "        # Backpropagation through each layer backwards (duh)\n",
    "        for i in range(1, self.layers - 1):\n",
    "            self.weights[-i] -= self.learning_rate * (self.layer_output[-i-1].T.dot(delta))\n",
    "            self.biases[-i] -= self.learning_rate * np.mean(delta, axis=0, keepdims=True)\n",
    "            delta = self.ReLU_derivative(self.layer_output[-i-1]) * (delta.dot(self.weights[-i].T))\n",
    "            # delta = deltas[-1].dot(self.weights[i+1].T) * self.ReLU_derivative(self.layer_output[i+1])\n",
    "            # deltas.append(delta)\n",
    "\n",
    "        # # Reversing deltas to match layer order\n",
    "        # deltas.reverse()\n",
    "\n",
    "        # # Updating weights and biases\n",
    "        # for i in range(self.layers - 1):\n",
    "        #     self.weights[i] -= self.learning_rate * self.layer_output[i].T.dot(deltas[i])\n",
    "        #     self.biases[i] -= self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y, epochs = 1000):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.feed_forward(X)\n",
    "            self.backpropagation(y, len(X))\n",
    "            #print(output)\n",
    "            if epoch % 1 == 0:\n",
    "                loss = -np.sum(y * np.log(output + 1e-9)) / X.shape[0]\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.feed_forward(X)\n",
    "        print(\"Output:\", output)\n",
    "        print(\"Label:\", np.argmax(output, axis=1))\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_new:\n",
    "    def __init__(self, input, hidden_layer1, hidden_layer2, output, learning_rate, sample_size):\n",
    "        self.input = input\n",
    "        self.hidden_layer1 = hidden_layer1\n",
    "        self.hidden_layer2 = hidden_layer2\n",
    "        self.output = output\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.sample_size = sample_size\n",
    "        self.total_error = 0\n",
    "\n",
    "        # Initializing weights and biases\n",
    "        self.weights1 = np.random.uniform(-1, 1, size=[input, hidden_layer1])\n",
    "        self.weights2 = np.random.uniform(-1, 1, size=[hidden_layer1, hidden_layer2])\n",
    "        self.weights3 = np.random.uniform(-1, 1, size=[hidden_layer2, output])\n",
    "\n",
    "        self.biases1 = np.zeros((1, hidden_layer1))\n",
    "        self.biases2 = np.zeros((1, hidden_layer2))\n",
    "        self.biases3 = np.zeros((1, output))\n",
    "\n",
    "        self.final_output = []\n",
    "        self.loss = []\n",
    "\n",
    "\n",
    "    # Defining useful functions\n",
    "    def sigmoid(self, x):\n",
    "        x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        return 1/(1+np.exp(-x_shifted * 1.0))\n",
    "        \n",
    "    def sigmoid_prime(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def ReLU_prime(self,x):\n",
    "        return (x>0)*1\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp = np.exp(x_shifted)\n",
    "        probabilities = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "    \n",
    "    def cross_entropy_loss(self, y):\n",
    "        loss = 0\n",
    "        # Loop through each sample\n",
    "        for sample in range(y.shape[0]):\n",
    "            # Get the predicted probabilities for the current sample\n",
    "            probabilities = self.final_output[sample]\n",
    "            \n",
    "            # Ensure numerical stability by adding a small constant (epsilon) to avoid log(0)\n",
    "            epsilon = 1e-9\n",
    "            probabilities = np.clip(probabilities, epsilon, 1 - epsilon)\n",
    "            \n",
    "            # Compute the loss for the current sample using the correct one-hot encoded label\n",
    "            loss += -np.sum(y[sample] * np.log(probabilities))\n",
    "        \n",
    "        # Return the average loss\n",
    "        return loss / y.shape[0]\n",
    "\n",
    "    # Forward\n",
    "    def train(self, X, y, epochs):\n",
    "        self.loss = np.zeros([epochs,1])\n",
    "        for epoch in range(epochs):\n",
    "            self.final_output = np.zeros([self.sample_size, self.output])\n",
    "            for sample, inputs in enumerate(X):\n",
    "                inputs = inputs.reshape(-1,1).T\n",
    "                # Forward pass\n",
    "                output1 = self.sigmoid(inputs.dot(self.weights1) + self.biases1)\n",
    "                output2 = self.sigmoid(output1.dot(self.weights2) + self.biases2)\n",
    "                output3 = self.softmax(output2.dot(self.weights3) + self.biases3)\n",
    "\n",
    "                self.final_output[sample] = output3\n",
    "\n",
    "                # Backward pass\n",
    "                error = output3 - y[sample]\n",
    "                self.total_error += error\n",
    "                \n",
    "                delta_out = error\n",
    "                self.weights3 -= self.learning_rate * (output2.T.dot(delta_out))\n",
    "                self.biases3 -= self.learning_rate * delta_out\n",
    "        \n",
    "                delta2 = self.sigmoid_prime(output2) * (delta_out.dot(self.weights3.T))\n",
    "                self.weights2 -= self.learning_rate * (output1.T.dot(delta2))\n",
    "                self.biases2 -= self.learning_rate * delta2\n",
    "\n",
    "                delta1 = self.sigmoid_prime(output1) * (delta2.dot(self.weights2.T))\n",
    "                self.weights1 -= self.learning_rate * (inputs.T.dot(delta1))\n",
    "                self.biases1 -= self.learning_rate * delta1\n",
    "\n",
    "            self.total_error = self.total_error/len(y)\n",
    "\n",
    "            self.loss[epoch] = self.cross_entropy_loss(y)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {self.loss[epoch]}, Final output: {self.final_output}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        self.final_output = np.zeros([self.sample_size, self.output])\n",
    "        for sample, inputs in enumerate(X):\n",
    "            # Forward pass\n",
    "            inputs = inputs.reshape(-1,1).T\n",
    "                # Forward pass\n",
    "            output1 = self.sigmoid(inputs.dot(self.weights1) + self.biases1)\n",
    "            output2 = self.sigmoid(output1.dot(self.weights2) + self.biases2)\n",
    "            output3 = self.softmax(output2.dot(self.weights3) + self.biases3)\n",
    "\n",
    "            self.final_output[sample] = output3\n",
    "\n",
    "            predictions.append(np.argmax(self.final_output[sample]))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"C:/Users/afrod/Documents/Neural_Networks/MergedDataset\"\n",
    "classes = [\"NonDemented\", \"VeryMildDemented\", \"MildDemented\", \"ModerateDemented\"]\n",
    "training_data = []\n",
    "\n",
    "\n",
    "def create_training_data():\n",
    "    for dementia_level in classes:\n",
    "        path = os.path.join(data_file, dementia_level)\n",
    "        class_num = classes.index(dementia_level)\n",
    "        for img in os.listdir(path):\n",
    "            # Convert to grayscale for smaller array dimensions\n",
    "            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "            final_array = cv2.resize(img_array, (50,47))\n",
    "            training_data.append([final_array, class_num])\n",
    "\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24230\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training_data)\n",
    "\n",
    "# Separating features and labels\n",
    "# Images are also flattened to be used as input in the knn algorithm\n",
    "X = np.array([features for features, _ in training_data]).reshape(-1, 50*47)\n",
    "y = np.array([label for _, label in training_data])\n",
    "\n",
    "# Rescaling\n",
    "X = X/X.max()\n",
    "\n",
    "# One-hot encoding\n",
    "y_onehot = np.zeros((y.size, int(y.max()) + 1))\n",
    "y_onehot[np.arange(y.size),y.astype(int)] = 1.0\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.4, random_state=42)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_new(2350, 100, 100, 4, 0.001, len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: [1.3886602], Final output: [[0.48388275 0.12694301 0.28514547 0.10402877]\n",
      " [0.36181644 0.12767838 0.41670766 0.09379751]\n",
      " [0.38558649 0.14663354 0.37741558 0.09036438]\n",
      " ...\n",
      " [0.23591263 0.26018052 0.29187884 0.212028  ]\n",
      " [0.26696231 0.28430223 0.26309477 0.18564069]\n",
      " [0.38328613 0.25992176 0.20999112 0.14680098]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afrod\\AppData\\Local\\Temp\\ipykernel_19844\\964597294.py:28: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x_shifted * 1.0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: [1.36150616], Final output: [[0.29151286 0.26891269 0.26177083 0.17780361]\n",
      " [0.29499867 0.26748832 0.26036553 0.17714748]\n",
      " [0.29339741 0.26616841 0.26378175 0.17665243]\n",
      " ...\n",
      " [0.29114728 0.27305349 0.25634224 0.17945699]\n",
      " [0.28960492 0.27169546 0.25973982 0.1789598 ]\n",
      " [0.29308501 0.27025127 0.25836218 0.17830153]]\n",
      "Epoch: 20, Loss: [1.36149841], Final output: [[0.29151708 0.26891155 0.26175025 0.17782112]\n",
      " [0.29499459 0.26749038 0.26034875 0.17716629]\n",
      " [0.29339758 0.26617356 0.26375699 0.17667187]\n",
      " ...\n",
      " [0.29115168 0.27304257 0.25633379 0.17947195]\n",
      " [0.28961331 0.27168783 0.25972348 0.17897537]\n",
      " [0.29308512 0.27024693 0.25834952 0.17831842]]\n",
      "Epoch: 30, Loss: [1.36149063], Final output: [[0.29152136 0.26891033 0.26172959 0.17783871]\n",
      " [0.29499054 0.26749237 0.2603319  0.1771852 ]\n",
      " [0.29339778 0.26617866 0.26373215 0.17669141]\n",
      " ...\n",
      " [0.29115615 0.27303154 0.25632532 0.17948698]\n",
      " [0.28962177 0.2716801  0.2597071  0.17899103]\n",
      " [0.29308528 0.2702425  0.25833682 0.1783354 ]]\n",
      "Epoch: 40, Loss: [1.36148283], Final output: [[0.29152569 0.26890904 0.26170887 0.1778564 ]\n",
      " [0.29498652 0.26749429 0.26031498 0.17720421]\n",
      " [0.29339802 0.2661837  0.26370722 0.17671106]\n",
      " ...\n",
      " [0.29116069 0.2730204  0.25631683 0.17950208]\n",
      " [0.28963031 0.27167226 0.25969066 0.17900677]\n",
      " [0.29308549 0.27023798 0.25832407 0.17835246]]\n"
     ]
    }
   ],
   "source": [
    "mlp.train(X_train, y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afrod\\AppData\\Local\\Temp\\ipykernel_19844\\964597294.py:28: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x_shifted * 1.0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP with layers [2350, 100, 100, 4], lr = 0.001, epochs = 50:\n",
      "Test Accuracy: 31.68%\n",
      "Accuracy: 0.31682555404234247\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      1.00      0.48      5118\n",
      "           1       0.00      0.00      0.00      4467\n",
      "           2       0.00      0.00      0.00      3937\n",
      "           3       0.00      0.00      0.00      2632\n",
      "\n",
      "    accuracy                           0.32     16154\n",
      "   macro avg       0.08      0.25      0.12     16154\n",
      "weighted avg       0.10      0.32      0.15     16154\n",
      "\n",
      "Confusion Matrix:\n",
      " [[5118    0    0    0]\n",
      " [4467    0    0    0]\n",
      " [3937    0    0    0]\n",
      " [2632    0    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afrod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\afrod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\afrod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on test set\n",
    "predictions = mlp.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(\"\\nMLP with layers [2350, 100, 100, 4], lr = 0.001, epochs = 50:\")\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.39312199],\n",
       "       [1.39214072],\n",
       "       [1.39088064],\n",
       "       [1.38959126],\n",
       "       [1.38830926],\n",
       "       [1.38704071],\n",
       "       [1.38579065],\n",
       "       [1.38456388],\n",
       "       [1.38336492],\n",
       "       [1.382198  ],\n",
       "       [1.38106678],\n",
       "       [1.37997431],\n",
       "       [1.37892297],\n",
       "       [1.37791449],\n",
       "       [1.37694998],\n",
       "       [1.37602983],\n",
       "       [1.37515392],\n",
       "       [1.37432167],\n",
       "       [1.373532  ],\n",
       "       [1.37083346],\n",
       "       [1.36671748],\n",
       "       [1.36617092],\n",
       "       [1.36552665],\n",
       "       [1.36492847],\n",
       "       [1.36442928],\n",
       "       [1.36387694],\n",
       "       [1.36323595],\n",
       "       [1.36285675],\n",
       "       [1.36249574],\n",
       "       [1.36206912],\n",
       "       [1.36160188],\n",
       "       [1.36118851],\n",
       "       [1.36084999],\n",
       "       [1.36046136],\n",
       "       [1.36023572],\n",
       "       [1.35992535],\n",
       "       [1.35973884],\n",
       "       [1.35957772],\n",
       "       [1.35962157],\n",
       "       [1.35946076],\n",
       "       [1.359187  ],\n",
       "       [1.35892768],\n",
       "       [1.35873198],\n",
       "       [1.35879359],\n",
       "       [1.3588937 ],\n",
       "       [1.35897807],\n",
       "       [1.35849076],\n",
       "       [1.35869301],\n",
       "       [1.35842554],\n",
       "       [1.35842598]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mat = np.matrix(mlp.loss)\n",
    "\n",
    "with open('n_100_0.01_50.txt','wb') as f:\n",
    "    for line in loss_mat:\n",
    "        np.savetxt(f, line, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afrod\\AppData\\Local\\Temp\\ipykernel_19844\\964597294.py:28: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x_shifted * 1.0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP with layers [2350, 100, 100, 4], lr = 0.001, epochs = 50:\n",
      "Train Accuracy: 31.70%\n",
      "Accuracy: 0.31704498555509697\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      1.00      0.48      7682\n",
      "           1       0.00      0.00      0.00      6733\n",
      "           2       0.00      0.00      0.00      5919\n",
      "           3       0.00      0.00      0.00      3896\n",
      "\n",
      "    accuracy                           0.32     24230\n",
      "   macro avg       0.08      0.25      0.12     24230\n",
      "weighted avg       0.10      0.32      0.15     24230\n",
      "\n",
      "Confusion Matrix:\n",
      " [[7682    0    0    0]\n",
      " [6733    0    0    0]\n",
      " [5919    0    0    0]\n",
      " [3896    0    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afrod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\afrod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\afrod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on test set\n",
    "predictions = mlp.predict(X_train)\n",
    "y_test_labels = np.argmax(y_train, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(\"\\nMLP with layers [2350, 100, 100, 4], lr = 0.001, epochs = 50:\")\n",
    "print(f'Train Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"Accuracy:\", accuracy_score(y_test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 2: Load CIFAR-10 dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m      5\u001b[0m ])\n\u001b[0;32m      7\u001b[0m trainset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m      8\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1000, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=2)\n",
    "\n",
    "def extract_features_labels(dataloader):\n",
    "    features, labels = [], []\n",
    "    for images, lbls in dataloader:\n",
    "        # Flatten images and append to list\n",
    "        features.extend(images.view(images.size(0), -1).numpy())\n",
    "        labels.extend(lbls.numpy())\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "train_features, train_labels = extract_features_labels(trainloader)\n",
    "test_features, test_labels = extract_features_labels(testloader)\n",
    "\n",
    "y_onehot_train = np.zeros((train_labels.size, int(train_labels.max()) + 1))\n",
    "y_onehot_train[np.arange(train_labels.size),train_labels.astype(int)] = 1.0\n",
    "\n",
    "y_onehot_test = np.zeros((test_labels.size, int(test_labels.max()) + 1))\n",
    "y_onehot_test[np.arange(test_labels.size),test_labels.astype(int)] = 1.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
